{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqhL5MiB/3tsn3SJfqCIJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devscoding/MAT421/blob/main/ModuleC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **19.1 Root Finding Problem statemnet**\n",
        "Root finding, as its name suggest is the process of finding the roots of a function, which is often times usefule for engineering, machine learning, and economic processing and other forms of data optimization.\n",
        "\n",
        "For some functions the roots may be simpler to find such as quadrtric functions in the form of $f(x) = ax^2 + bx + c$ which we then use the quadratic formula $x =\\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ to find that our function has two roots.\n",
        "\n",
        "However, for more complicated functions the process of finding roots is much more demanding and thus a problem arises.\n",
        "\n",
        "Oftentimes it can be dificult to determine the exact roots for a function which is why it can be useful to generate **numerical approximations** for the roots of our more complex functions"
      ],
      "metadata": {
        "id": "zWRwTU9-cl86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the root of f(x) = tan(x) - x near x = Ï€ (around 3.14)\n",
        "import numpy as np\n",
        "from scipy import optimize\n",
        "\n",
        "f = lambda x: np.tan(x) - x\n",
        "r = optimize.fsolve(f, 3.14)\n",
        "print('r =', r)\n",
        "\n",
        "# Verifying the result\n",
        "result = f(r)\n",
        "print('result =', result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NehSYM0dho0g",
        "outputId": "adc47471-f3a2-4793-f021-a581fb918a2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r = [1.21069579e-08]\n",
            "result = [0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see our result is very close to zero suggesting we have a good approximation of a root\n",
        "\n",
        "In other instances such as the function $e^x + 1$ which has no roots *f_solve* will still try to find an approximation but will fail"
      ],
      "metadata": {
        "id": "6r4e0vcEnjv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying to use fsolve to find the roots of a function with no roots\n",
        "f = lambda x: np.exp(x) + 1\n",
        "\n",
        "r, infodict, ier, mesg = optimize.fsolve(f, 0, full_output=True)\n",
        "\n",
        "print('r =', r)\n",
        "print('result =', f(r))\n",
        "print(mesg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGQMZ8WgoP6W",
        "outputId": "61d59f7a-43e4-4491-cec1-69bd47452419"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r = [-34.16936449]\n",
            "result = [1.]\n",
            "The iteration is not making good progress, as measured by the \n",
            "  improvement from the last ten iterations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **19.2 Tolerance**\n",
        "We define **error** as being the deviation from our desired/expected value, whereas **tolerance** is the level of error accpetable to be satisfied with our computed value.\n",
        "\n",
        "Additionally we say that a program **converged** to a solution if the found solution has an error less than the tolerance.\n",
        "\n",
        "In the context of computing roots we want values such that the function evaluated at said value is close to zero. Thus $e = |f(x)|$ and $e = |x_{i+1}-x_{i}|$ are seen as possible measures of error.\n",
        "\n",
        "It is important to note that the use of tolerances is based off the functions being used."
      ],
      "metadata": {
        "id": "1QZDOI8BqWT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **19.3 Bisection Method**\n",
        "The **Intermediate Value Theorem** states that if $f(x)$ is a continuous function between $a$ and $b$ and sign$f(a)\\neq$ sign$f(b)$ then there exits a $c$ such that $a < c < b$ and $f(c)=0$\n",
        "\n",
        "The **bisection method** utilizes the conditions of the intermediate value theorem to evalute roots of functions by stating it must exist on the open interval $(a, b)$\n",
        "\n",
        "Let $m$ be the midpoint on the open interval $(a,b)$ then if $f(m) = 0$ or approximately then through the bisection method we say that $m$ is a root. Additionally if $f(m)> 0$ then $a$ is the root on the open interval $(m,b)$ and if $f(m) < 0$ then $b$ is the root on the open interval $(a,m)$."
      ],
      "metadata": {
        "id": "7TJ7Iv_MveGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the bisection method to calculate roots\n",
        "import numpy as np\n",
        "\n",
        "def my_bisection(f, a, b, tol):\n",
        "    # Check if a and b bound a root\n",
        "    if np.sign(f(a)) == np.sign(f(b)):\n",
        "        raise Exception(\n",
        "         \"The scalars a and b do not bound a root\")\n",
        "\n",
        "    # Midpoint\n",
        "    m = (a + b)/2\n",
        "\n",
        "    if np.abs(f(m)) < tol:\n",
        "        return m\n",
        "    elif np.sign(f(a)) == np.sign(f(m)):\n",
        "        return my_bisection(f, m, b, tol)\n",
        "    elif np.sign(f(b)) == np.sign(f(m)):\n",
        "        return my_bisection(f, a, m, tol)\n",
        "\n",
        "# Bisection method on the function tan(x) - x\n",
        "f = lambda x: np.tan(x) - x\n",
        "\n",
        "# Applying the bisection method between 0 and 1.5 since that is where tan(x) is continuous\n",
        "r1 = my_bisection(f, 0, 1.5, 0.1)  # Approximate root with tolerance 0.1\n",
        "print(\"r1 =\", r1)\n",
        "\n",
        "r01 = my_bisection(f, 0, 1.5, 0.01)  # More precise root with tolerance 0.01\n",
        "print(\"r01 =\", r01)\n",
        "\n",
        "# Verifying functions\n",
        "print(\"f(r1) =\", f(r1))\n",
        "print(\"f(r01) =\", f(r01))"
      ],
      "metadata": {
        "id": "tQL7oygSsFMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e46499e8-60b4-40d8-f543-83874bf63b44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r1 = 0.375\n",
            "r01 = 0.1875\n",
            "f(r1) = 0.01862657592563277\n",
            "f(r01) = 0.002228610718059132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see both $r_1$ and $r_{01}$ give approximations of possible roots that fall within our tolerance, however, we can see that $r_{01}$ is a much better approximation as the value for $f(r_{01})$ is much closer to $0$ then $f(r_1)$."
      ],
      "metadata": {
        "id": "cqI5lX5fBOfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **19.4 Newton-Raphson Method**\n",
        "When considering a function $f(x)$ to be a smooth continous function there exist another way to approximate roots from methods previously discussed.\n",
        "Let $x_r$ be our uknown root of $f(x)$ and $x_0$ be our intial guess for $x_r$ which in most cases will not be our root so the goal is to find an $x_1$ that is an improved guess that is found by taking the linear approximation of $f(x)$ around $x_0$ and finding a $f(x_1) = 0$. Thus from our linear approximation we are left with the equation: $x_1= x_0 - \\frac{f(x_0)}{f^\\prime(x_0)}$\n",
        "\n",
        "Each additional imporved guess is referred to as a **Newton-step** which makes a better guess based off the previous and is given by the equation $x_i= x_{i-1} - \\frac{g(x_{i-1})}{g^\\prime(x_{i-1})}$. The **Newton-Raphson Method** finds roots by iterating Newton steps form $x_0$ until the error is found to be less than the tolerance"
      ],
      "metadata": {
        "id": "7P4-Ww4YCbHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Newton-Raphson Method on the fuction x^2 - 4\n",
        "\n",
        "# Function and derivative\n",
        "f = lambda x: x**2 - 4\n",
        "f_prime = lambda x: 2*x\n",
        "\n",
        "# Manual Newton-Raphson\n",
        "newton_raphson = 3 - (f(3)) / (f_prime(3))  # Starting at x0 = 3\n",
        "print(\"newton_raphson =\", newton_raphson)\n",
        "print(\"Actual root =\", 2)\n",
        "\n",
        "# Newton-Raphson function\n",
        "def my_newton(f, df, x0, tol):\n",
        "    if abs(f(x0)) < tol:\n",
        "        return x0\n",
        "    else:\n",
        "        return my_newton(f, df, x0 - f(x0)/df(x0), tol)\n",
        "\n",
        "# Applying function with a starting point of 3 and tolerance of 1e-6\n",
        "estimate = my_newton(f, f_prime, 3, 1e-6)\n",
        "print(\"estimate =\", estimate)"
      ],
      "metadata": {
        "id": "C7xXxrZQEFZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc74df5b-628f-4151-d2ae-997a2f224e1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newton_raphson = 2.1666666666666665\n",
            "Actual root = 2\n",
            "estimate = 2.000000000026214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see our approximated value from the Newton-Raphson method is extremly close to our known root of 2, thus proving its usefulness. It is important to know that the Newton-Raphson method does have limitations, for example if the derivative of our initial guess is close to 0, then the Newton step will be very large and produce a value that is potentially far away from the actual root, giving us an inaccurate approximation."
      ],
      "metadata": {
        "id": "ci3w6_1unEOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **19.5 Root Finding in Python**\n",
        "Thankfully Python has functions for root finding that make each procees simplier, for example we can use *f_solve* from *scipy.optimize*. The function $2x^2 -3x -2$ has roots at $-\\frac{1}{2}$ and $2$ so we will use *f_solve* to show this"
      ],
      "metadata": {
        "id": "TuHYySrWoMWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import fsolve\n",
        "\n",
        "# Function 2x^2 - 3x - 2\n",
        "f = lambda x: 2*x**2 -3*x - 2\n",
        "roots = fsolve(f, [0,2])\n",
        "\n",
        "# Print results\n",
        "print(roots)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITAkj60xorWc",
        "outputId": "42c9b945-dfc0-42ef-adb1-c248a6978be5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.5  2. ]\n"
          ]
        }
      ]
    }
  ]
}